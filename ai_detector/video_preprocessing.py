"""
AI Video Detection Pipeline - Pre-screening Module

This module provides fast detection of AI-generated videos through:
1. Metadata parsing (C2PA, creation tools, encoding tags)
2. Frame preprocessing with OCR and logo detection
3. Invisible watermark detection (placeholder)
4. Comprehensive output logic

Usage:
    from ai_detector.video_preprocessing import AIVideoPreprocessor
    
    preprocessor = AIVideoPreprocessor()
    result = preprocessor.analyze_video("path/to/video.mp4")
    
    if result["final_decision"] != "Unknown":
        # Video flagged as AI-generated, skip ML analysis
        return result
    else:
        # Pass to ML classifier for deeper analysis
        return ml_classifier.analyze(video)
"""

import os
import cv2
import json
import hashlib
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import logging
from pathlib import Path
import tempfile
import subprocess
from dataclasses import dataclass
from datetime import datetime

# OCR imports
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    logging.warning("EasyOCR not available. Install with: pip install easyocr")

try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    logging.warning("Pytesseract not available. Install with: pip install pytesseract")

# Image processing
from skimage.metrics import structural_similarity as ssim
from PIL import Image
import imagehash

# Metadata parsing
try:
    import exifread
    EXIFREAD_AVAILABLE = True
except ImportError:
    EXIFREAD_AVAILABLE = False
    logging.warning("ExifRead not available. Install with: pip install ExifRead")

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DetectionResult:
    """Structured result from AI video detection."""
    metadata_flag: bool
    metadata_details: Dict[str, Any]
    ocr_flag: bool
    ocr_detections: List[str]
    logo_flag: bool
    logo_detections: List[str]
    invisible_watermark_conf: float
    invisible_watermark_details: Dict[str, Any]
    final_decision: str
    confidence_score: float
    processing_time: float
    frames_analyzed: int

class AIVideoPreprocessor:
    """
    Comprehensive AI video detection pipeline for pre-screening.
    
    This class implements a multi-stage detection system that can quickly
    identify AI-generated videos through various indicators before requiring
    expensive ML classification.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize the AI video preprocessor.
        
        Args:
            config: Optional configuration dictionary
        """
        self.config = config or {}
        
        # Initialize OCR readers
        self.easyocr_reader = None
        self.tesseract_available = TESSERACT_AVAILABLE
        
        if EASYOCR_AVAILABLE:
            try:
                self.easyocr_reader = easyocr.Reader(['en'], gpu=False)
                logger.info("EasyOCR initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize EasyOCR: {e}")
        
        # Known AI generation indicators
        self.ai_metadata_indicators = {
            'creation_tools': [
                'openai', 'sora', 'dall-e', 'midjourney', 'stable diffusion',
                'adobe firefly', 'runway', 'pika labs', 'invideo', 'synthesia',
                'deepfake', 'faceswap', 'first order motion', 'wav2lip',
                'real-esrgan', 'topaz', 'ai upscaler', 'waifu2x'
            ],
            'software_tags': [
                'ai generated', 'artificial intelligence', 'machine learning',
                'neural network', 'deep learning', 'synthetic media',
                'computer generated', 'algorithmically created'
            ],
            'encoding_hints': [
                'ai_generated', 'synthetic', 'deepfake', 'ml_enhanced',
                'ai_upscaled', 'style_transfer', 'face_swap'
            ]
        }
        
        # OCR watermark patterns
        self.watermark_patterns = [
            'ai generated', 'ai-generated', 'artificial intelligence',
            '© openai', 'openai', 'sora', 'dall-e', 'dall·e',
            'midjourney', 'stable diffusion', 'adobe firefly',
            'runway ml', 'pika labs', 'invideo', 'synthesia',
            'created with ai', 'generated by ai', 'ai creation',
            'deepfake', 'synthetic media', 'computer generated',
            'not real', 'artificial', 'machine learning',
            'neural network', 'ai model', 'ai tool'
        ]
        
        # Logo detection setup
        self.logo_templates = self._load_logo_templates()
        
        logger.info("AI Video Preprocessor initialized")
    
    def _load_logo_templates(self) -> Dict[str, np.ndarray]:
        """Load logo templates for detection."""
        templates = {}
        
        # Create simple synthetic templates for common AI logos
        # In production, you'd load actual logo images
        logo_dir = Path("ai_detector/logo_templates")
        logo_dir.mkdir(exist_ok=True)
        
        # Create placeholder templates (you should replace with actual logos)
        template_configs = {
            'openai_logo': (100, 30, [0, 0, 0]),  # Black rectangle
            'midjourney_logo': (80, 80, [128, 128, 128]),  # Gray square
            'runway_logo': (120, 40, [255, 0, 0]),  # Red rectangle
        }
        
        for name, (width, height, color) in template_configs.items():
            template_path = logo_dir / f"{name}.png"
            if not template_path.exists():
                # Create simple colored rectangle as placeholder
                template = np.full((height, width, 3), color, dtype=np.uint8)
                cv2.imwrite(str(template_path), template)
            
            templates[name] = cv2.imread(str(template_path))
        
        return templates
    
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """
        Main analysis function for AI video detection.
        
        Args:
            video_path: Path to the video file
            
        Returns:
            Detection result dictionary
        """
        start_time = datetime.now()
        
        try:
            # Stage 1: Metadata Analysis
            logger.info(f"Analyzing video: {video_path}")
            metadata_result = self._analyze_metadata(video_path)
            
            # Early exit if metadata indicates AI generation
            if metadata_result['flag']:
                result = DetectionResult(
                    metadata_flag=True,
                    metadata_details=metadata_result['details'],
                    ocr_flag=False,
                    ocr_detections=[],
                    logo_flag=False,
                    logo_detections=[],
                    invisible_watermark_conf=0.0,
                    invisible_watermark_details={},
                    final_decision="AI-generated (metadata)",
                    confidence_score=0.95,
                    processing_time=(datetime.now() - start_time).total_seconds(),
                    frames_analyzed=0
                )
                return self._format_result(result)
            
            # Stage 2: Frame Extraction and Analysis
            frames = self._extract_key_frames(video_path)
            if not frames:
                logger.warning("No frames could be extracted from video")
                return self._create_error_result("Frame extraction failed")
            
            # Stage 3: OCR Analysis
            ocr_result = self._analyze_frames_ocr(frames)
            
            # Early exit if OCR detects AI indicators
            if ocr_result['flag']:
                result = DetectionResult(
                    metadata_flag=False,
                    metadata_details=metadata_result['details'],
                    ocr_flag=True,
                    ocr_detections=ocr_result['detections'],
                    logo_flag=False,
                    logo_detections=[],
                    invisible_watermark_conf=0.0,
                    invisible_watermark_details={},
                    final_decision="AI-generated (watermark)",
                    confidence_score=0.90,
                    processing_time=(datetime.now() - start_time).total_seconds(),
                    frames_analyzed=len(frames)
                )
                return self._format_result(result)
            
            # Stage 4: Logo Detection
            logo_result = self._analyze_frames_logos(frames)
            
            # Early exit if logos detected
            if logo_result['flag']:
                result = DetectionResult(
                    metadata_flag=False,
                    metadata_details=metadata_result['details'],
                    ocr_flag=False,
                    ocr_detections=ocr_result['detections'],
                    logo_flag=True,
                    logo_detections=logo_result['detections'],
                    invisible_watermark_conf=0.0,
                    invisible_watermark_details={},
                    final_decision="AI-generated (logo)",
                    confidence_score=0.85,
                    processing_time=(datetime.now() - start_time).total_seconds(),
                    frames_analyzed=len(frames)
                )
                return self._format_result(result)
            
            # Stage 5: Invisible Watermark Detection (Placeholder)
            watermark_result = self._detect_invisible_watermarks(frames)
            
            # Final decision logic
            if watermark_result['confidence'] > 0.7:
                final_decision = "AI-generated (invisible watermark)"
                confidence = watermark_result['confidence']
            else:
                final_decision = "Unknown"
                confidence = 0.0
            
            result = DetectionResult(
                metadata_flag=False,
                metadata_details=metadata_result['details'],
                ocr_flag=False,
                ocr_detections=ocr_result['detections'],
                logo_flag=False,
                logo_detections=logo_result['detections'],
                invisible_watermark_conf=watermark_result['confidence'],
                invisible_watermark_details=watermark_result['details'],
                final_decision=final_decision,
                confidence_score=confidence,
                processing_time=(datetime.now() - start_time).total_seconds(),
                frames_analyzed=len(frames)
            )
            
            return self._format_result(result)
            
        except Exception as e:
            logger.error(f"Error analyzing video {video_path}: {e}")
            return self._create_error_result(str(e))
    
    def _analyze_metadata(self, video_path: str) -> Dict[str, Any]:
        """
        Analyze video metadata for AI generation indicators.
        
        Args:
            video_path: Path to video file
            
        Returns:
            Dictionary with flag and details
        """
        result = {'flag': False, 'details': {}}
        
        try:
            # Method 1: FFprobe metadata extraction
            ffprobe_data = self._extract_ffprobe_metadata(video_path)
            if ffprobe_data:
                result['details']['ffprobe'] = ffprobe_data
                
                # Check for AI indicators in metadata
                metadata_text = json.dumps(ffprobe_data).lower()
                for category, indicators in self.ai_metadata_indicators.items():
                    for indicator in indicators:
                        if indicator in metadata_text:
                            result['flag'] = True
                            result['details']['detected_indicator'] = indicator
                            result['details']['category'] = category
                            logger.info(f"AI indicator found in metadata: {indicator}")
                            return result
            
            # Method 2: EXIF data analysis (for containers that support it)
            if EXIFREAD_AVAILABLE:
                exif_data = self._extract_exif_data(video_path)
                if exif_data:
                    result['details']['exif'] = exif_data
                    
                    # Check EXIF for AI indicators
                    exif_text = str(exif_data).lower()
                    for category, indicators in self.ai_metadata_indicators.items():
                        for indicator in indicators:
                            if indicator in exif_text:
                                result['flag'] = True
                                result['details']['detected_indicator'] = indicator
                                result['details']['category'] = f"exif_{category}"
                                logger.info(f"AI indicator found in EXIF: {indicator}")
                                return result
            
            # Method 3: C2PA manifest detection (placeholder)
            c2pa_result = self._check_c2pa_manifest(video_path)
            if c2pa_result:
                result['details']['c2pa'] = c2pa_result
                if c2pa_result.get('ai_generated', False):
                    result['flag'] = True
                    result['details']['detected_indicator'] = 'c2pa_ai_manifest'
                    result['details']['category'] = 'provenance'
                    logger.info("AI generation detected via C2PA manifest")
                    return result
            
        except Exception as e:
            logger.warning(f"Metadata analysis failed: {e}")
            result['details']['error'] = str(e)
        
        return result
    
    def _extract_ffprobe_metadata(self, video_path: str) -> Optional[Dict]:
        """Extract metadata using ffprobe."""
        try:
            cmd = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                '-show_streams',
                video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            if result.returncode == 0:
                return json.loads(result.stdout)
        except Exception as e:
            logger.warning(f"FFprobe extraction failed: {e}")
        
        return None
    
    def _extract_exif_data(self, video_path: str) -> Optional[Dict]:
        """Extract EXIF data if available."""
        try:
            with open(video_path, 'rb') as f:
                tags = exifread.process_file(f)
                return {str(key): str(value) for key, value in tags.items()}
        except Exception as e:
            logger.warning(f"EXIF extraction failed: {e}")
        
        return None
    
    def _check_c2pa_manifest(self, video_path: str) -> Optional[Dict]:
        """
        Check for C2PA manifest (placeholder implementation).
        
        In production, you would use c2pa-python or similar library.
        """
        # Placeholder implementation
        # TODO: Integrate with c2pa-python when available
        
        try:
            # Mock C2PA check - replace with actual implementation
            file_size = os.path.getsize(video_path)
            
            # Simple heuristic: very large files might have embedded manifests
            if file_size > 100 * 1024 * 1024:  # 100MB
                return {
                    'manifest_found': False,
                    'ai_generated': False,
                    'provenance_chain': [],
                    'note': 'C2PA integration placeholder'
                }
        except Exception as e:
            logger.warning(f"C2PA check failed: {e}")
        
        return None
    
    def _extract_key_frames(self, video_path: str, num_frames: int = 3) -> List[np.ndarray]:
        """
        Extract key frames from video (start, middle, end).
        
        Args:
            video_path: Path to video file
            num_frames: Number of frames to extract
            
        Returns:
            List of frame arrays
        """
        frames = []
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"Could not open video: {video_path}")
                return frames
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames == 0:
                logger.error("Video has no frames")
                return frames
            
            # Calculate frame positions
            if num_frames == 3:
                positions = [0, total_frames // 2, total_frames - 1]
            else:
                positions = [int(i * total_frames / num_frames) for i in range(num_frames)]
            
            for pos in positions:
                cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
                ret, frame = cap.read()
                if ret:
                    frames.append(frame)
                    logger.debug(f"Extracted frame at position {pos}")
            
            cap.release()
            logger.info(f"Extracted {len(frames)} frames from video")
            
        except Exception as e:
            logger.error(f"Frame extraction failed: {e}")
        
        return frames
    
    def _analyze_frames_ocr(self, frames: List[np.ndarray]) -> Dict[str, Any]:
        """
        Analyze frames for watermark text using OCR.
        
        Args:
            frames: List of frame arrays
            
        Returns:
            Dictionary with flag and detections
        """
        result = {'flag': False, 'detections': []}
        
        for i, frame in enumerate(frames):
            try:
                # Convert to grayscale for better OCR
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # Enhance contrast for watermark detection
                enhanced = cv2.convertScaleAbs(gray, alpha=2.0, beta=0)
                
                # Try EasyOCR first
                text_detections = []
                if self.easyocr_reader:
                    try:
                        ocr_results = self.easyocr_reader.readtext(enhanced)
                        for (bbox, text, confidence) in ocr_results:
                            if float(confidence) > 0.5:  # Filter low confidence
                                text_detections.append(text.lower())
                    except Exception as e:
                        logger.warning(f"EasyOCR failed on frame {i}: {e}")
                
                # Fallback to Tesseract
                if not text_detections and self.tesseract_available:
                    try:
                        text = pytesseract.image_to_string(enhanced).lower()
                        if text.strip():
                            text_detections.extend(text.split())
                    except Exception as e:
                        logger.warning(f"Tesseract failed on frame {i}: {e}")
                
                # Check for watermark patterns
                for text in text_detections:
                    for pattern in self.watermark_patterns:
                        if pattern in text:
                            result['flag'] = True
                            result['detections'].append(f"Frame {i}: '{text}' (pattern: '{pattern}')")
                            logger.info(f"Watermark detected in frame {i}: {pattern}")
            
            except Exception as e:
                logger.warning(f"OCR analysis failed for frame {i}: {e}")
        
        return result
    
    def _analyze_frames_logos(self, frames: List[np.ndarray]) -> Dict[str, Any]:
        """
        Analyze frames for known AI tool logos.
        
        Args:
            frames: List of frame arrays
            
        Returns:
            Dictionary with flag and detections
        """
        result = {'flag': False, 'detections': []}
        
        for i, frame in enumerate(frames):
            try:
                # Convert to grayscale for template matching
                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                for logo_name, template in self.logo_templates.items():
                    if template is None:
                        continue
                    
                    # Convert template to grayscale
                    gray_template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
                    
                    # Template matching
                    res = cv2.matchTemplate(gray_frame, gray_template, cv2.TM_CCOEFF_NORMED)
                    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)
                    
                    # Threshold for logo detection
                    if max_val > 0.7:  # Adjust threshold as needed
                        result['flag'] = True
                        result['detections'].append(f"Frame {i}: {logo_name} (confidence: {max_val:.3f})")
                        logger.info(f"Logo detected in frame {i}: {logo_name}")
                    
                    # Alternative: Use image hashing for logo detection
                    frame_hash = imagehash.phash(Image.fromarray(gray_frame))
                    template_hash = imagehash.phash(Image.fromarray(gray_template))
                    hash_diff = frame_hash - template_hash
                    
                    if hash_diff < 10:  # Similar hashes
                        result['flag'] = True
                        result['detections'].append(f"Frame {i}: {logo_name} (hash similarity)")
                        logger.info(f"Logo detected via hash in frame {i}: {logo_name}")
            
            except Exception as e:
                logger.warning(f"Logo analysis failed for frame {i}: {e}")
        
        return result
    
    def _detect_invisible_watermarks(self, frames: List[np.ndarray]) -> Dict[str, Any]:
        """
        Detect invisible watermarks (placeholder implementation).
        
        Args:
            frames: List of frame arrays
            
        Returns:
            Dictionary with confidence and details
        """
        result = {'confidence': 0.0, 'details': {}}
        
        try:
            # Placeholder for SynthID or similar invisible watermark detection
            # This would integrate with Google's SynthID API or similar service
            
            # Mock implementation - replace with actual watermark detection
            for i, frame in enumerate(frames):
                # Simulate watermark detection analysis
                # In reality, this would call an API or use a specialized library
                
                # Simple frequency domain analysis as placeholder
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                f_transform = np.fft.fft2(gray)
                f_shift = np.fft.fftshift(f_transform)
                magnitude_spectrum = np.log(np.abs(f_shift) + 1)
                
                # Look for unusual patterns in frequency domain
                # This is a very basic heuristic - real watermark detection is much more sophisticated
                high_freq_energy = np.mean(magnitude_spectrum[magnitude_spectrum.shape[0]//4:3*magnitude_spectrum.shape[0]//4,
                                                           magnitude_spectrum.shape[1]//4:3*magnitude_spectrum.shape[1]//4])
                
                # Normalize and use as mock confidence
                normalized_energy = min(1.0, high_freq_energy / 10.0)
                
                if normalized_energy > result['confidence']:
                    result['confidence'] = normalized_energy
                    result['details'][f'frame_{i}'] = {
                        'high_freq_energy': float(high_freq_energy),
                        'analysis_method': 'frequency_domain_placeholder'
                    }
            
            # Add API integration placeholder
            result['details']['api_integration'] = {
                'synthid_available': False,
                'custom_detector_available': False,
                'note': 'Placeholder implementation - integrate with actual watermark detection service'
            }
            
            logger.info(f"Invisible watermark analysis complete. Max confidence: {result['confidence']:.3f}")
            
        except Exception as e:
            logger.warning(f"Invisible watermark detection failed: {e}")
            result['details']['error'] = str(e)
        
        return result
    
    def _format_result(self, result: DetectionResult) -> Dict[str, Any]:
        """Format the detection result as a dictionary."""
        return {
            'metadata_flag': result.metadata_flag,
            'metadata_details': result.metadata_details,
            'ocr_flag': result.ocr_flag,
            'ocr_detections': result.ocr_detections,
            'logo_flag': result.logo_flag,
            'logo_detections': result.logo_detections,
            'invisible_watermark_conf': result.invisible_watermark_conf,
            'invisible_watermark_details': result.invisible_watermark_details,
            'final_decision': result.final_decision,
            'confidence_score': result.confidence_score,
            'processing_time': result.processing_time,
            'frames_analyzed': result.frames_analyzed,
            'timestamp': datetime.now().isoformat()
        }
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """Create an error result dictionary."""
        return {
            'metadata_flag': False,
            'metadata_details': {'error': error_message},
            'ocr_flag': False,
            'ocr_detections': [],
            'logo_flag': False,
            'logo_detections': [],
            'invisible_watermark_conf': 0.0,
            'invisible_watermark_details': {},
            'final_decision': 'Error',
            'confidence_score': 0.0,
            'processing_time': 0.0,
            'frames_analyzed': 0,
            'error': error_message,
            'timestamp': datetime.now().isoformat()
        }

# Integration function for existing pipeline
def preprocess_video_for_ai_detection(video_path: str) -> Dict[str, Any]:
    """
    Convenience function for integrating with existing video analysis pipeline.
    
    Args:
        video_path: Path to video file
        
    Returns:
        Detection result dictionary
    """
    preprocessor = AIVideoPreprocessor()
    return preprocessor.analyze_video(video_path)

# Example usage and testing
if __name__ == "__main__":
    # Test the preprocessor
    preprocessor = AIVideoPreprocessor()
    
    # Example usage
    test_video = "path/to/test/video.mp4"
    if os.path.exists(test_video):
        result = preprocessor.analyze_video(test_video)
        print(json.dumps(result, indent=2))
    else:
        print("Test video not found. Create a test video to run the analysis.")
        
        # Show example result format
        example_result = {
            "metadata_flag": True,
            "metadata_details": {
                "detected_indicator": "openai",
                "category": "creation_tools",
                "ffprobe": {"format": {"tags": {"creation_tool": "OpenAI Sora"}}}
            },
            "ocr_flag": False,
            "ocr_detections": [],
            "logo_flag": False,
            "logo_detections": [],
            "invisible_watermark_conf": 0.0,
            "invisible_watermark_details": {},
            "final_decision": "AI-generated (metadata)",
            "confidence_score": 0.95,
            "processing_time": 1.23,
            "frames_analyzed": 0,
            "timestamp": "2024-01-15T10:30:00"
        }
        
        print("Example result format:")
        print(json.dumps(example_result, indent=2))